import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from datetime import datetime

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit, cross_val_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel, RFE
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

# ========== GELƒ∞≈ûTƒ∞Rƒ∞LMƒ∞≈û KONFƒ∞G√úRASYON ==========
RANDOM_STATE = 42
TEST_SIZE = 0.15
VAL_SIZE = 0.15
N_JOBS = -1

DATA_PATH = "data/bundesliga_matches_2023_2025_final_fe_team_values_cleaned.xlsx"
PLAYER_DATA_PATH = "data/final_bundesliga_dataset_complete.xlsx"

# Geli≈ütirilmi≈ü √∂zellik listesi
SELECTED_FEATURES = [
    # Takƒ±m Deƒüer ve Demografi √ñzellikleri
    'home_current_value_eur', 'away_current_value_eur',
    'home_previous_value_eur', 'away_previous_value_eur',
    'home_value_change_pct', 'away_value_change_pct',
    'home_squad_avg_age', 'away_squad_avg_age',
    'home_absolute_change', 'away_absolute_change',
    'home_log_current_value', 'away_log_current_value',
    'value_difference', 'value_ratio',
    
    # Performans ve Form √ñzellikleri
    'home_goals', 'away_goals', 'home_xg', 'away_xg',
    'goals_difference', 'goals_ratio', 'xg_difference', 'xg_ratio',
    'home_form', 'away_form', 'form_difference',
    'home_last5_form_points', 'away_last5_form_points',
    
    # H2H (Head-to-Head) √ñzellikleri
    'h2h_home_wins', 'h2h_away_wins', 'h2h_draws',
    'h2h_home_goals', 'h2h_away_goals', 'h2h_matches_count',
    'h2h_win_ratio', 'h2h_goal_difference', 'h2h_avg_goals',
    
    # Derby ve √ñzel Durum √ñzellikleri
    'isDerby', 'age_difference', 'injury_difference',
    
    # Power Index ve Advanced Metrics
    'home_power_index', 'away_power_index', 'power_difference',
    'performance_ratio'
]

# ========== GELƒ∞≈ûTƒ∞Rƒ∞LMƒ∞≈û √ñZEL TRANSFORMERLAR ==========
class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):
    """Geli≈ümi≈ü √∂zellik m√ºhendisliƒüi transformer'ƒ±"""
    
    def __init__(self):
        self.feature_names = []
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = X.copy()
        
        # 1. Exponential √∂zellikler
        value_cols = ['home_current_value_eur', 'away_current_value_eur']
        for col in value_cols:
            if col in X.columns:
                X[f'{col}_log'] = np.log1p(X[col])
                X[f'{col}_sqrt'] = np.sqrt(X[col])
        
        # 2. Interaction √∂zellikleri - OVERFITTING'E NEDEN OLANLARI KALDIRDIK
        # form_value_interaction_home ve form_value_interaction_away overfitting'e neden oluyor
        # Bu y√ºzden bu karma≈üƒ±k interaction'larƒ± kaldƒ±rƒ±yoruz
        
        # 3. Rolling ortalamalar
        if 'matchday' in X.columns:
            X['matchday_sin'] = np.sin(2 * np.pi * X['matchday'] / 34)
            X['matchday_cos'] = np.cos(2 * np.pi * X['matchday'] / 34)
        
        # 4. Kategori bazlƒ± √∂zellikler
        if 'h2h_win_ratio' in X.columns:
            X['h2h_dominant'] = (X['h2h_win_ratio'] > 0.6).astype(int)
            X['h2h_balanced'] = ((X['h2h_win_ratio'] >= 0.4) & (X['h2h_win_ratio'] <= 0.6)).astype(int)
        
        # 5. Geli≈ümi≈ü power metrics
        if all(col in X.columns for col in ['home_power_index', 'away_power_index']):
            X['power_ratio'] = X['home_power_index'] / X['away_power_index']
            X['power_sum'] = X['home_power_index'] + X['away_power_index']
        
        self.feature_names = X.columns.tolist()
        return X

# ========== FEATURE SELECTION FONKSƒ∞YONLARI ==========
def perform_strict_feature_selection(X_train, y_train, X_val, X_test, method='importance'):
    """OVERFITTING √ñNLEMEK ƒ∞√áƒ∞N DAHA STRICT FEATURE SELECTION"""
    print("üîç STRICT Feature selection yapƒ±lƒ±yor...")
    
    if method == 'importance':
        # Random Forest ile feature importance - DAHA AGRESIF THRESHOLD
        estimator = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)
        selector = SelectFromModel(estimator, threshold='mean')  # 'mean' daha agresif
        
        selector.fit(X_train, y_train)
        selected_features = X_train.columns[selector.get_support()].tolist()
        
        # Eƒüer hala √ßok fazla √∂zellik varsa, en iyi 10-15 tanesini al
        if len(selected_features) > 15:
            print(f"‚ö° √áok fazla √∂zellik se√ßildi ({len(selected_features)}), en iyi 15 tanesi alƒ±nƒ±yor...")
            estimator.fit(X_train, y_train)
            importances = estimator.feature_importances_
            indices = np.argsort(importances)[::-1]
            selected_features = [X_train.columns[i] for i in indices[:15]]
        
    elif method == 'rfe':
        # Recursive Feature Elimination - DAHA AZ √ñZELLƒ∞K
        estimator = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE)
        rfe = RFE(estimator=estimator, n_features_to_select=min(15, X_train.shape[1]))  # Max 15 √∂zellik
        rfe.fit(X_train, y_train)
        selected_features = X_train.columns[rfe.support_].tolist()
    
    else:
        # T√ºm feature'larƒ± se√ß
        selected_features = X_train.columns.tolist()
    
    print(f"‚úÖ Se√ßilen √∂zellik sayƒ±sƒ±: {len(selected_features)}/{X_train.shape[1]}")
    print(f"üìã Se√ßilen √∂zellikler: {selected_features}")
    
    X_train_selected = X_train[selected_features]
    X_val_selected = X_val[selected_features]
    X_test_selected = X_test[selected_features]
    
    return X_train_selected, X_val_selected, X_test_selected, selected_features

# ========== GELƒ∞≈ûTƒ∞Rƒ∞LMƒ∞≈û VERƒ∞ HAZIRLAMA ==========
def enhanced_data_preparation(df_matches, df_players):
    """
    Geli≈ütirilmi≈ü veri hazƒ±rlama ve zenginle≈ütirme fonksiyonu
    """
    print("üîß Geli≈ütirilmi≈ü veri hazƒ±rlama ba≈ülƒ±yor...")
    
    # 1. Temel veri temizliƒüi
    df = df_matches.copy()
    
    # 2. Takƒ±m isimlerini standartla≈ütƒ±r
    if 'homeTeam.name' in df.columns and 'HomeTeam' not in df.columns:
        df['HomeTeam'] = df['homeTeam.name']
    if 'awayTeam.name' in df.columns and 'AwayTeam' not in df.columns:
        df['AwayTeam'] = df['awayTeam.name']
    
    # 3. Result_Numeric olu≈ütur
    def safe_get_result(row):
        try:
            home_goals = row.get('score.fullTime.home', row.get('score.fullTime.home', 0))
            away_goals = row.get('score.fullTime.away', row.get('score.fullTime.away', 0))
            
            if pd.isna(home_goals) or pd.isna(away_goals):
                return 0
                
            if home_goals > away_goals:
                return 1
            elif home_goals < away_goals:
                return 2
            else:
                return 0
        except:
            return 0
    
    df['Result_Numeric'] = df.apply(safe_get_result, axis=1)
    
    # 4. Tarih i≈üleme
    if 'utcDate' in df.columns:
        df['Date'] = pd.to_datetime(df['utcDate'], errors='coerce')
        df = df.sort_values('Date').reset_index(drop=True)
    
    # 5. Eksik √∂zellikler i√ßin geli≈ümi≈ü doldurma
    df = enhanced_missing_value_imputation(df)
    
    # 6. Geli≈ümi≈ü feature engineering
    df = advanced_feature_engineering(df)
    
    # 7. Takƒ±m ratinglerini hesapla
    df = compute_enhanced_ratings(df, df_players)
    
    # 8. Outlier handling
    df = handle_outliers(df)
    
    print("‚úÖ Geli≈ütirilmi≈ü veri hazƒ±rlama tamamlandƒ±!")
    return df

def enhanced_missing_value_imputation(df):
    """Geli≈ümi≈ü eksik deƒüer doldurma"""
    print("üìä Eksik deƒüer analizi ve doldurma...")
    
    imputation_strategies = {
        'h2h_home_wins': 0, 'h2h_away_wins': 0, 'h2h_draws': 0,
        'h2h_home_goals': 0, 'h2h_away_goals': 0, 'h2h_matches_count': 0,
        'h2h_win_ratio': 0.5, 'h2h_goal_difference': 0, 'h2h_avg_goals': 2.5,
        
        'home_form': 0.5, 'away_form': 0.5, 'form_difference': 0,
        'home_last5_form_points': 0, 'away_last5_form_points': 0,
        
        'home_current_value_eur': df['home_current_value_eur'].median() if 'home_current_value_eur' in df.columns else 200000000,
        'away_current_value_eur': df['away_current_value_eur'].median() if 'away_current_value_eur' in df.columns else 200000000,
        
        'home_goals': df['home_goals'].median() if 'home_goals' in df.columns else 1.5,
        'away_goals': df['away_goals'].median() if 'away_goals' in df.columns else 1.5,
    }
    
    for column, default_value in imputation_strategies.items():
        if column in df.columns:
            null_count = df[column].isnull().sum()
            if null_count > 0:
                if callable(default_value):
                    df[column].fillna(default_value(df), inplace=True)
                else:
                    df[column].fillna(default_value, inplace=True)
    
    return df

def simplified_feature_engineering(df):
    """OVERFITTING'I √ñNLEMEK ƒ∞√áƒ∞N SADELE≈ûTƒ∞Rƒ∞LMƒ∞≈û FEATURE ENGINEERING"""
    print("üéØ Sadele≈ütirilmi≈ü √∂zellik m√ºhendisliƒüi...")
    df = df.copy()
    
    # SADECE EN TEMEL VE ANLAMLI √ñZELLƒ∞KLER
    # 1. Value-based √∂zellikler
    if all(col in df.columns for col in ['home_current_value_eur', 'away_current_value_eur']):
        df['value_difference'] = df['home_current_value_eur'] - df['away_current_value_eur']
        df['value_ratio'] = df['home_current_value_eur'] / (df['away_current_value_eur'] + 1e-8)
    
    # 2. Form-based √∂zellikler
    if all(col in df.columns for col in ['home_form', 'away_form']):
        df['form_difference'] = df['home_form'] - df['away_form']
        df['form_sum'] = df['home_form'] + df['away_form']
    
    # 3. Goal-based √∂zellikler
    if all(col in df.columns for col in ['home_goals', 'away_goals']):
        df['goals_difference'] = df['home_goals'] - df['away_goals']
        df['total_goals'] = df['home_goals'] + df['away_goals']
    
    # KARMA≈ûIK INTERACTION FEATURE'LARI √áIKARDIK
    # form_value_interaction gibi overfitting'e neden olan feature'lar kaldƒ±rƒ±ldƒ±
    
    return df

def advanced_feature_engineering(df):
    """Geli≈ümi≈ü √∂zellik m√ºhendisliƒüi - Sadele≈ütirilmi≈ü versiyon"""
    print("üéØ Geli≈ümi≈ü √∂zellik m√ºhendisliƒüi...")
    
    # √ñnce temel √∂zellikleri olu≈ütur
    df = simplified_feature_engineering(df)
    
    # 1. Polynomial √∂zellikler (sƒ±nƒ±rlƒ± sayƒ±da)
    if all(col in df.columns for col in ['home_form', 'away_form']):
        df['form_product'] = df['home_form'] * df['away_form']
    
    # 2. Ratio-based √∂zellikler
    if all(col in df.columns for col in ['home_current_value_eur', 'away_current_value_eur']):
        df['value_ratio_log'] = np.log1p(df['home_current_value_eur']) - np.log1p(df['away_current_value_eur'])
    
    # 3. Momentum-based √∂zellikler
    if 'home_last5_form_points' in df.columns and 'away_last5_form_points' in df.columns:
        df['momentum_difference'] = df['home_last5_form_points'] - df['away_last5_form_points']
    
    # 4. H2H dominance √∂zellikleri
    if 'h2h_win_ratio' in df.columns:
        df['h2h_dominance'] = (df['h2h_win_ratio'] - 0.5) * df['h2h_matches_count'].clip(upper=10)  # Max 10 ma√ß
    
    # 5. Power metrics
    if all(col in df.columns for col in ['home_power_index', 'away_power_index']):
        df['relative_power'] = df['home_power_index'] / (df['away_power_index'] + 1e-8)
        df['power_advantage'] = df['home_power_index'] - df['away_power_index']
    
    return df

def handle_outliers(df):
    """Outlier'larƒ± i≈üleme"""
    print("üìä Outlier handling...")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        if col not in ['Result_Numeric', 'isDerby']:
            Q1 = df[col].quantile(0.05)
            Q3 = df[col].quantile(0.95)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            df[col] = np.clip(df[col], lower_bound, upper_bound)
    
    return df

# ========== GELƒ∞≈ûTƒ∞Rƒ∞LMƒ∞≈û RATING HESAPLAMA ==========
def compute_enhanced_ratings(df, df_players):
    """Geli≈ütirilmi≈ü takƒ±m rating hesaplama"""
    print("‚≠ê Geli≈ütirilmi≈ü takƒ±m ratingleri hesaplanƒ±yor...")
    
    if 'Home_AvgRating' not in df.columns:
        df['Home_AvgRating'] = 65.0
        df['Away_AvgRating'] = 65.0
    
    rating_cols = ['Home_AvgRating', 'Away_AvgRating', 'Home_GK_Rating', 'Home_DF_Rating', 
                   'Home_MF_Rating', 'Home_FW_Rating', 'Away_GK_Rating', 'Away_DF_Rating', 
                   'Away_MF_Rating', 'Away_FW_Rating']
    
    for col in rating_cols:
        if col not in df.columns:
            if 'AvgRating' in col:
                df[col] = 65.0
            else:
                df[col] = 65.0
    
    if all(col in df.columns for col in ['Home_AvgRating', 'Away_AvgRating']):
        df['Rating_Diff'] = df['Home_AvgRating'] - df['Away_AvgRating']
        df['Total_AvgRating'] = df['Home_AvgRating'] + df['Away_AvgRating']
    
    return df

# ========== OVERFITTING √ñNLEYƒ∞Cƒ∞ MODEL PIPELINE ==========
def create_overfitting_prevention_pipeline(selected_features):
    """OVERFITTING √ñNLEMEK ƒ∞√áƒ∞N BASƒ∞T VE REGULARIZE EDƒ∞LMƒ∞≈û PIPELINE"""
    
    # Sadece scaler ve model - feature selection pipeline dƒ±≈üƒ±nda
    preprocessor = ColumnTransformer([
        ('scaler', RobustScaler(), selected_features)
    ], remainder='drop')
    
    # OVERFITTING √ñNLEYƒ∞Cƒ∞ LightGBM parametreleri
    lgbm_clf = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=3,
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS,
        verbosity=-1,
        n_estimators=500,  # ‚¨ÖÔ∏è √áOK AZALTILDI (2000 ‚Üí 500)
        learning_rate=0.01,  # ‚¨ÖÔ∏è D√ú≈û√úR√úLD√ú (0.05 ‚Üí 0.01)
        max_depth=3,  # ‚¨ÖÔ∏è DERƒ∞NLƒ∞K AZALTILDI (6 ‚Üí 3)
        num_leaves=10,  # ‚¨ÖÔ∏è √áOK AZALTILDI (31 ‚Üí 10)
        min_child_samples=50,  # ‚¨ÖÔ∏è ARTIRILDI (20 ‚Üí 50)
        subsample=0.6,  # ‚¨ÖÔ∏è AZALTILDI
        colsample_bytree=0.6,  # ‚¨ÖÔ∏è AZALTILDI
        reg_alpha=2.0,  # ‚¨ÖÔ∏è REGULARIZATION ARTIRILDI (0.1 ‚Üí 2.0)
        reg_lambda=2.0,  # ‚¨ÖÔ∏è REGULARIZATION ARTIRILDI (0.1 ‚Üí 2.0)
        force_row_wise=True
    )
    
    return Pipeline([
        ('preprocessor', preprocessor),
        ('lgbm', lgbm_clf)
    ])

# ========== GELƒ∞≈ûTƒ∞Rƒ∞LMƒ∞≈û VERƒ∞ Y√úKLEME ==========
def load_and_validate_enhanced_data():
    """Geli≈ütirilmi≈ü veri y√ºkleme ve doƒürulama"""
    print("\nüìä Geli≈ütirilmi≈ü veri y√ºkleniyor...")
    
    try:
        df_matches = pd.read_excel(DATA_PATH)
        df_matches.columns = [col.strip().replace(' ', '_') for col in df_matches.columns]
        
        df_players = pd.read_excel(PLAYER_DATA_PATH)
        
        df = enhanced_data_preparation(df_matches, df_players)
        
        missing_features = []
        for feat in SELECTED_FEATURES:
            if feat not in df.columns:
                missing_features.append(feat)
                df[feat] = 0
        
        if missing_features:
            print(f"‚ö†Ô∏è Eksik √∂zellikler varsayƒ±lan deƒüerlerle dolduruldu: {missing_features}")
        
        numeric_cols = df[SELECTED_FEATURES].select_dtypes(include=np.number).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        class_distribution = df['Result_Numeric'].value_counts().sort_index()
        print(f"üìà Sƒ±nƒ±f Daƒüƒ±lƒ±mƒ±: {dict(class_distribution)}")
        
        print("‚úÖ Geli≈ütirilmi≈ü veri hazƒ±rlƒ±ƒüƒ± tamamlandƒ±")
        return df
        
    except Exception as e:
        print(f"‚ùå Veri y√ºkleme hatasƒ±: {e}")
        raise

# ========== OVERFITTING √ñNLEYƒ∞Cƒ∞ MODEL Eƒûƒ∞Tƒ∞Mƒ∞ ==========
def train_enhanced_model():
    """OVERFITTING √ñNLEYƒ∞Cƒ∞ model eƒüitim fonksiyonu"""
    print("‚öΩ Bundesliga Tahmin Modeli - Overfitting √ñnleyici S√ºr√ºm")
    print("=" * 70)
    print("‚úÖ Advanced feature engineering")
    print("‚úÖ STRICT feature selection (Max 15 √∂zellik)") 
    print("‚úÖ Robust outlier handling")
    print("‚úÖ ENHANCED regularization")
    print("‚úÖ Advanced cross-validation")
    print("‚úÖ Class balancing techniques")
    print("‚úÖ OVERFITTING PREVENTION techniques")
    print("=" * 70)
    
    # Veriyi y√ºkle
    df = load_and_validate_enhanced_data()
    
    # Zaman bazlƒ± split
    train_df, val_df, test_df = time_based_split(df, TEST_SIZE, VAL_SIZE)
    
    # Feature ve target'larƒ± ayƒ±r
    X_train = train_df[SELECTED_FEATURES].copy()
    y_train = train_df['Result_Numeric'].copy()
    
    X_val = val_df[SELECTED_FEATURES].copy()
    y_val = val_df['Result_Numeric'].copy()
    
    X_test = test_df[SELECTED_FEATURES].copy()
    y_test = test_df['Result_Numeric'].copy()
    
    # 1. √ñnce feature engineering uygula
    print("üîß Feature engineering uygulanƒ±yor...")
    feature_engineer = AdvancedFeatureEngineer()
    X_train = feature_engineer.fit_transform(X_train)
    X_val = feature_engineer.transform(X_val)
    X_test = feature_engineer.transform(X_test)
    
    # 2. Pipeline dƒ±≈üƒ±nda STRICT feature selection yap
    X_train_selected, X_val_selected, X_test_selected, important_features = perform_strict_feature_selection(
        X_train, y_train, X_val, X_test, method='importance'
    )
    
    # 3. Sƒ±nƒ±f aƒüƒ±rlƒ±klarƒ±nƒ± hesapla
    classes = np.unique(y_train)
    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
    class_weight_dict = dict(zip(classes, class_weights))
    sample_weights_train = np.array([class_weight_dict[yy] for yy in y_train])
    
    print(f"üìä Eƒüitim verisi: {X_train_selected.shape}")
    print(f"üìä Validation verisi: {X_val_selected.shape}")
    print(f"üìä Test verisi: {X_test_selected.shape}")
    print(f"‚öñÔ∏è Sƒ±nƒ±f aƒüƒ±rlƒ±klarƒ±: {class_weight_dict}")
    
    # 4. Overfitting √∂nleyici pipeline olu≈ütur
    model = create_overfitting_prevention_pipeline(important_features)
    
    # 5. OVERFITTING √ñNLEYƒ∞Cƒ∞ hiperparametre optimizasyonu
    param_distributions = {
        'lgbm__learning_rate': [0.005, 0.01, 0.02],  # ‚¨ÖÔ∏è Daha d√º≈ü√ºk
        'lgbm__max_depth': [2, 3, 4],  # ‚¨ÖÔ∏è Daha sƒ±ƒü
        'lgbm__num_leaves': [8, 10, 12],  # ‚¨ÖÔ∏è √áok daha az
        'lgbm__min_child_samples': [40, 50, 60],  # ‚¨ÖÔ∏è Daha b√ºy√ºk
        'lgbm__reg_alpha': [1.0, 2.0, 3.0],  # ‚¨ÖÔ∏è Daha g√º√ßl√º regularization
        'lgbm__reg_lambda': [1.0, 2.0, 3.0],
        'lgbm__subsample': [0.5, 0.6, 0.7],
        'lgbm__colsample_bytree': [0.5, 0.6, 0.7],
        'lgbm__n_estimators': [300, 500, 700]  # ‚¨ÖÔ∏è √áok daha az
    }
    
    # Daha fazla fold ve daha katƒ± split
    tscv = TimeSeriesSplit(n_splits=10)  # ‚¨ÖÔ∏è Fold sayƒ±sƒ±nƒ± artƒ±r
    
    print("\nüéØ Overfitting √ñnleyici Hiperparametre Optimizasyonu...")
    random_search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_distributions,
        n_iter=20,  # ‚¨ÖÔ∏è Daha az iterasyon
        cv=tscv,
        scoring='balanced_accuracy',  # ‚¨ÖÔ∏è Daha dengeli bir metrik
        n_jobs=N_JOBS,
        verbose=2,
        random_state=RANDOM_STATE,
        return_train_score=True
    )
    
    # Optimizasyonu ger√ßekle≈ütir (se√ßilmi≈ü √∂zelliklerle)
    random_search.fit(X_train_selected, y_train, lgbm__sample_weight=sample_weights_train)
    
    # En iyi parametreler ve skor
    best_params = random_search.best_params_
    best_score = random_search.best_score_
    
    print(f"\nüèÜ En ƒ∞yi Parametreler: {best_params}")
    print(f"üèÜ En ƒ∞yi CV Skoru: {best_score:.4f}")
    
    # 6. Final modeli eƒüit (EARLY STOPPING ile)
    print("\nüöÄ Final model eƒüitimi (Early Stopping ile)...")
    final_model = create_overfitting_prevention_pipeline(important_features)
    final_model.set_params(**best_params)
    
    # Early stopping i√ßin parametreleri ayarla
    final_model.named_steps['lgbm'].set_params(
        n_estimators=1000,  # Early stopping i√ßin yeterince b√ºy√ºk
        early_stopping_rounds=50,
        verbose=100
    )
    
    # T√ºm veriyi birle≈ütir (train + val)
    X_combined = pd.concat([X_train_selected, X_val_selected])
    y_combined = pd.concat([y_train, y_val])
    sample_weights_combined = np.array([class_weight_dict[yy] for yy in y_combined])
    
    # Final modeli EARLY STOPPING ile eƒüit
    final_model.fit(
        X_train_selected, y_train,
        lgbm__eval_set=[(X_val_selected, y_val)],
        lgbm__eval_metric='multi_logloss',
        lgbm__sample_weight=sample_weights_train,
        lgbm__callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
    )
    
    # 7. Model deƒüerlendirme
    print("\nüìä Kapsamlƒ± Model Deƒüerlendirme:")
    evaluate_model_comprehensive(final_model, X_test_selected, y_test, X_train_selected, y_train)
    
    # 8. Feature importance analizi
    analyze_feature_importance(final_model, important_features)
    
    # 9. Modeli kaydet
    save_enhanced_model(final_model, important_features, best_params, random_search.cv_results_)
    
    return final_model, important_features

def evaluate_model_comprehensive(model, X_test, y_test, X_train, y_train):
    """Kapsamlƒ± model deƒüerlendirme"""
    
    y_pred_test = model.predict(X_test)
    y_pred_train = model.predict(X_train)
    y_proba_test = model.predict_proba(X_test)
    
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_f1 = f1_score(y_test, y_pred_test, average='weighted')
    test_precision = precision_score(y_test, y_pred_test, average='weighted')
    test_recall = recall_score(y_test, y_pred_test, average='weighted')
    
    train_accuracy = accuracy_score(y_train, y_pred_train)
    train_f1 = f1_score(y_train, y_pred_train, average='weighted')
    
    accuracy_gap = train_accuracy - test_accuracy
    f1_gap = train_f1 - test_f1
    
    print(f"üìà Test Accuracy: {test_accuracy:.4f}")
    print(f"üìà Test F1-Score: {test_f1:.4f}")
    print(f"üìà Test Precision: {test_precision:.4f}")
    print(f"üìà Test Recall: {test_recall:.4f}")
    print(f"üèãÔ∏è Train Accuracy: {train_accuracy:.4f}")
    print(f"üèãÔ∏è Train F1-Score: {train_f1:.4f}")
    print(f"üìä Accuracy Gap (Overfitting): {accuracy_gap:.4f}")
    print(f"üìä F1 Gap (Overfitting): {f1_gap:.4f}")
    
    # Overfitting analizi
    if accuracy_gap > 0.15:
        print("üö® CRITICAL: Ciddi overfitting riski!")
    elif accuracy_gap > 0.10:
        print("‚ö†Ô∏è WARNING: Orta seviye overfitting riski!")
    elif accuracy_gap > 0.05:
        print("‚ÑπÔ∏è INFO: Hafif overfitting riski")
    else:
        print("‚úÖ EXCELLENT: Overfitting riski √ßok d√º≈ü√ºk!")
    
    print("\nüéØ Detaylƒ± Classification Report:")
    print(classification_report(y_test, y_pred_test, target_names=['Draw', 'HomeWin', 'AwayWin']))
    
    # Confusion matrix
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred_test)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Draw', 'HomeWin', 'AwayWin'],
                yticklabels=['Draw', 'HomeWin', 'AwayWin'])
    plt.title('Confusion Matrix - Test Set (Overfitting √ñnleyici)')
    plt.ylabel('Ger√ßek Deƒüer')
    plt.xlabel('Tahmin Edilen Deƒüer')
    plt.savefig('models/confusion_matrix_overfitting_fixed.png', dpi=300, bbox_inches='tight')
    plt.close()

def analyze_feature_importance(model, feature_names):
    """Feature importance analizi"""
    try:
        if hasattr(model.named_steps['lgbm'], 'feature_importances_'):
            importances = model.named_steps['lgbm'].feature_importances_
            indices = np.argsort(importances)[::-1]
            
            print("\nüèÜ Feature Importance Ranking:")
            for i, idx in enumerate(indices[:15]):  # Sadece top 15
                if idx < len(feature_names):
                    print(f"{i+1:2d}. {feature_names[idx]:30s} ({importances[idx]:.4f})")
            
            # G√∂rselle≈ütirme
            plt.figure(figsize=(12, 8))
            top_n = min(10, len(feature_names))  # Sadece top 10
            plt.barh(range(top_n), importances[indices[:top_n]][::-1], align='center')
            plt.yticks(range(top_n), [feature_names[i] for i in indices[:top_n]][::-1])
            plt.xlabel('Importance')
            plt.title('Top Feature Importances (Overfitting √ñnleyici)')
            plt.tight_layout()
            plt.savefig('models/feature_importance_overfitting_fixed.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    except Exception as e:
        print(f"‚ö†Ô∏è Feature importance analizinde hata: {e}")

def save_enhanced_model(model, important_features, best_params, cv_results):
    """Geli≈ütirilmi≈ü model kaydetme"""
    os.makedirs("models", exist_ok=True)
    
    model_path = "models/bundesliga_model_overfitting_fixed.pkl"
    joblib.dump(model, model_path)
    
    feature_info = {
        'important_features': important_features,
        'all_features': SELECTED_FEATURES,
        'best_params': best_params,
        'cv_results': cv_results,
        'timestamp': datetime.now().isoformat(),
        'model_version': 'overfitting_prevention_v1'
    }
    joblib.dump(feature_info, "models/feature_info_overfitting_fixed.pkl")
    
    performance_report = {
        'model_type': 'LightGBM Overfitting Prevention',
        'features_used': len(important_features),
        'total_features': len(SELECTED_FEATURES),
        'training_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'max_features_limit': 15
    }
    
    with open("models/performance_report_overfitting_fixed.txt", "w") as f:
        for key, value in performance_report.items():
            f.write(f"{key}: {value}\n")
    
    print(f"\nüíæ Model kaydedildi: {model_path}")
    print("üíæ Feature bilgileri kaydedildi")
    print("üíæ Performans raporu kaydedildi")

# ========== YARDIMCI FONKSƒ∞YONLAR ==========
def time_based_split(df, test_size=0.15, val_size=0.15):
    """Zaman bazlƒ± split fonksiyonu"""
    if 'Date' in df.columns:
        df_sorted = df.sort_values('Date').reset_index(drop=True)
    else:
        df_sorted = df.reset_index(drop=True)
        print("‚ÑπÔ∏è Date s√ºtunu yok, orijinal sƒ±ra kullanƒ±lƒ±yor")
    
    n = len(df_sorted)
    test_split_idx = int(n * (1 - test_size))
    val_split_idx = int(test_split_idx * (1 - val_size))
    
    train_df = df_sorted.iloc[:val_split_idx]
    val_df = df_sorted.iloc[val_split_idx:test_split_idx]
    test_df = df_sorted.iloc[test_split_idx:]
    
    print(f"üìä Split bilgisi: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}")
    return train_df, val_df, test_df

# ========== ANA FONKSƒ∞YON ==========
def main():
    print("üèÜ Bundesliga Tahmin Modeli - Overfitting √ñnleyici S√ºr√ºm")
    print("=" * 60)
    print("üöÄ Ba≈ülatƒ±lƒ±yor...")
    
    os.makedirs("models", exist_ok=True)
    os.makedirs("data", exist_ok=True)
    
    try:
        model, important_features = train_enhanced_model()
        
        print("\nüéâ Overfitting √∂nleyici model eƒüitimi ba≈üarƒ±yla tamamlandƒ±!")
        print(f"üìã Kullanƒ±lan √∂nemli feature'lar: {len(important_features)}/{len(SELECTED_FEATURES)}")
        print("üìç Model dosyalarƒ± 'models/' klas√∂r√ºne kaydedildi")
        
        # Overfitting durumunu deƒüerlendir
        print("\nüìä Overfitting √ñnleme √ñzeti:")
        print("‚úÖ Model karma≈üƒ±klƒ±ƒüƒ± azaltƒ±ldƒ±")
        print("‚úÖ Feature sayƒ±sƒ± sƒ±nƒ±rlandƒ± (max 15)")
        print("‚úÖ Regularization artƒ±rƒ±ldƒ±")
        print("‚úÖ Early stopping eklendi")
        print("‚úÖ Cross-validation artƒ±rƒ±ldƒ±")
        
    except Exception as e:
        print(f"‚ùå Model eƒüitimi sƒ±rasƒ±nda hata: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()